{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6aT-G6wRDJx"
      },
      "source": [
        "##### I. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWts3C16QJ5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "096a2da2-f24e-407a-ae28-6c851a1fecfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar (child): spark-3.5.5-bin-hadoop3.tgz: Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ],
      "source": [
        "# Java 11 and Spark 3.5.5 installation\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz\n",
        "!tar -xzf spark-3.5.5-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyydIe8WIxmw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import findspark\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from google.colab import files\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import TimestampType\n",
        "from functools import reduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uz6QU7XCTJNE"
      },
      "outputs": [],
      "source": [
        "# Setup environment\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.5-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR45EixHSFDM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "2b1531f7-61dc-4708-f6a5-84d893fdf693"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/spark-3.5.5-bin-hadoop3/./bin/spark-submit'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-fc805937112a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initial SparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DataCleaning\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                     \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m                     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m             )\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             self._do_init(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mpopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"preexec_fn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreexec_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;31m# preexec_fn not supported on Windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1024\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m   1027\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1953\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merr_filename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/spark-3.5.5-bin-hadoop3/./bin/spark-submit'"
          ]
        }
      ],
      "source": [
        "# Initial SparkSession\n",
        "findspark.init()\n",
        "spark = SparkSession.builder.appName(\"DataCleaning\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "820jcF79T1qj"
      },
      "source": [
        "##### II. Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owZxjgnnHiA6"
      },
      "outputs": [],
      "source": [
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuPmwEVuUD_4"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv(\"OnlineRetail.csv\", header=True, inferSchema=True)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcXr0ypiZj7g"
      },
      "source": [
        "##### III. Data Profiling, Anomaly Detection, and Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_lo4k-TbT-0"
      },
      "source": [
        "###### 3.1 Check the number of columns and rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3JxQiRDYRbq"
      },
      "outputs": [],
      "source": [
        "# Check the number of columns and rows\n",
        "print(\"Number of columns: \", len(df.columns))\n",
        "print(\"Number of rows: \", df.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBO3d9OCcjoy"
      },
      "source": [
        "###### 3.2 Abnormal 1: Check and Correct data type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tibO5YTHcROn"
      },
      "outputs": [],
      "source": [
        "# Check schema\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pT-hH4woct8L"
      },
      "outputs": [],
      "source": [
        "# Convert datatype of InvoiceDate to timestamp\n",
        "df_cleaned = df.withColumn(\"InvoiceDate\", to_timestamp(\"InvoiceDate\", \"M/d/yyyy H:mm\"))\n",
        "\n",
        "# Re-check schema\n",
        "df_cleaned.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMMgGe7vbY3F"
      },
      "source": [
        "###### 3.3 Abnormal 2: Check and handle missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0DwAIoYaAPI"
      },
      "outputs": [],
      "source": [
        "# Check for missing values in original data\n",
        "missing_values = df_cleaned.select([\n",
        "    count(when(col(c).isNull(), c)).alias(c) for c in df_cleaned.columns\n",
        "])\n",
        "missing_values.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyTPhbTRe7IU"
      },
      "source": [
        "- `Description`: 1,454 --> remove since it is hard to guess the product description\n",
        "- `CustomerID`: 135,080 --> might keep them, change `null` value into `Unknown`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkBKyUMpD4Ey"
      },
      "outputs": [],
      "source": [
        "df_cleaned.filter(df.Description.isNull()).select(\"StockCode\").distinct().count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Val6h85mE0Rw"
      },
      "outputs": [],
      "source": [
        "missing_stockcodes = (\n",
        "    df_cleaned\n",
        "    .filter(col(\"Description\").isNull())\n",
        "    .select(\"StockCode\")\n",
        "    .distinct()\n",
        "    .rdd.flatMap(lambda x: x)\n",
        "    .collect()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKnFQg0LfG28"
      },
      "outputs": [],
      "source": [
        "# Delete the null values in Description\n",
        "df_cleaned = df_cleaned.filter(df_cleaned.Description.isNotNull())\n",
        "\n",
        "# Change the value in CustomerID into \"Unknown\" for null values\n",
        "df_cleaned = df_cleaned.withColumn(\"CustomerID\", when(df_cleaned.CustomerID.isNull(), \"Unknown\").otherwise(df_cleaned.CustomerID))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83XxLar-gU6p"
      },
      "outputs": [],
      "source": [
        "# Re-check for missing values in the cleaned data\n",
        "missing_values = df_cleaned.select([\n",
        "    count(when(col(c).isNull(), c)).alias(c) for c in df_cleaned.columns\n",
        "])\n",
        "missing_values.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2qIuAq3hNtb"
      },
      "source": [
        "###### 3.4 Abnormal 3: Check and handle duplicate rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf1VB8qqiuYR"
      },
      "outputs": [],
      "source": [
        "# Check the number of duplicate rows\n",
        "duplicate_count = df_cleaned.count() - df_cleaned.dropDuplicates().count()\n",
        "print(\"Number of duplicate rows: \", duplicate_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjVXXu36iT2J"
      },
      "outputs": [],
      "source": [
        "# Drop duplicate rows\n",
        "df_cleaned = df_cleaned.dropDuplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ob5UJFthntV"
      },
      "outputs": [],
      "source": [
        "# Re-check the number of duplicate rows\n",
        "duplicate_count = df_cleaned.count() - df_cleaned.dropDuplicates().count()\n",
        "print(\"Number of duplicate rows: \", duplicate_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE_yn_5ljkWU"
      },
      "source": [
        "###### 3.5 Abnormal 4: Handle negative Quantity and UnitPrice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70XRIfIsKCtY"
      },
      "source": [
        "**Check the number of cancelled order:**\n",
        "\n",
        "The InvoiceNo starts with C (has negative Quantity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4emt9eNMIqSh"
      },
      "outputs": [],
      "source": [
        "cancel = df_cleaned.filter(col(\"InvoiceNo\").startswith(\"C\"))\n",
        "print(\"Number of canceled orders: \", cancel.count())\n",
        "cancel.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axBh6xlxbqom"
      },
      "source": [
        "**Check for invalid values in Quantity and UnitPrice:**\n",
        "- Quantity < 0\n",
        "- UnitPrice < 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kbb9cCsQbCnY"
      },
      "outputs": [],
      "source": [
        "# Quantity has negative value\n",
        "negative_Quantity = df_cleaned.filter(col(\"Quantity\") < 0).count()\n",
        "print(\"Number of negative Quantity: \", negative_Quantity)\n",
        "df_cleaned.filter(col(\"Quantity\") < 0).select(\"InvoiceNo\", \"Quantity\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qrv-nTBKcZlr"
      },
      "outputs": [],
      "source": [
        "# UnitPrice has negative value\n",
        "negative_UnitPrice = df_cleaned.filter(col(\"UnitPrice\") < 0).count()\n",
        "print(\"Number of negative UnitPrice: \", negative_UnitPrice)\n",
        "df_cleaned.filter(col(\"UnitPrice\") < 0).select(\"InvoiceNo\", \"UnitPrice\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AQ5gCmNKq5_"
      },
      "source": [
        "- We can see that cancelled orders also have the negative Quantity.\n",
        "- The dataset contains 9,251 cancelled orders, while 9,725 orders have a negative quantity.\n",
        "- Therefore, in the Data Cleaning section, when we remove orders with negative quantities, we also remove the cancelled orders at the same time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "882LybgNlVqU"
      },
      "source": [
        "**Remove abnormal Quantity and UnitPrice**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCYd0Z04kShu"
      },
      "outputs": [],
      "source": [
        "# Filter out negative Quantity and UnitPrice\n",
        "df_cleaned = df_cleaned.filter((col(\"Quantity\") > 0) & (col(\"UnitPrice\") > 0))\n",
        "\n",
        "# Check negative Quantity and UnitPrice again\n",
        "print(\"Negative Quantity count:\", df_cleaned.filter(col(\"Quantity\") < 0).count())\n",
        "print(\"Negative UnitPrice count:\", df_cleaned.filter(col(\"UnitPrice\") < 0).count())\n",
        "\n",
        "# Check number of cancelled orders again\n",
        "cancel = df_cleaned.filter(col(\"InvoiceNo\").startswith(\"C\"))\n",
        "print(\"Number of cancelled orders: \", cancel.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UghVhdfmTP7p"
      },
      "source": [
        "###### 3.6 Abnormal 5: Identify abnormal `StockCode` - `Description` pairs that are not actual products"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FktCSM6m5yi"
      },
      "source": [
        "**Check abnormal StockCodes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyT3oD9MTdAV"
      },
      "outputs": [],
      "source": [
        "excluded_stockcodes = [\"POST\", \"DOT\", \"M\", \"C2\", \"BANK CHARGES\",\"S\", \"B\", \"AMAZONFEE\",\n",
        "                       \"gift_0001_10\", \"gift_0001_20\",\"gift_0001_30\",\"gift_0001_40\",\"gift_0001_50\"]\n",
        "\n",
        "# Identify rows with exclued StockCode\n",
        "df_excluded = df_cleaned.filter(col(\"StockCode\").isin(excluded_stockcodes))\n",
        "\n",
        "# Show distinct excluded StockCode - Description pairs\n",
        "df_excluded.select(\"StockCode\", \"Description\").distinct().show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWayWBCgnIrj"
      },
      "source": [
        "**Handle abnormal `StockCode` and `Description` pairs that are not actual products**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbtSMhRCm4VL"
      },
      "outputs": [],
      "source": [
        "df_cleaned = df_cleaned.filter(~col(\"StockCode\").isin(excluded_stockcodes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqPhtm84nZnr"
      },
      "outputs": [],
      "source": [
        "# Re-check the abnormal stock code\n",
        "df_excluded = df_cleaned.filter(col(\"StockCode\").isin(excluded_stockcodes))\n",
        "df_excluded.select(\"StockCode\", \"Description\").distinct().show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_7yh_bZnzSk"
      },
      "source": [
        "##### IV. Data Cleaning results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQa3u3PSn4QK"
      },
      "outputs": [],
      "source": [
        "# The number of rows before cleaning\n",
        "rows_before_cleaning = df.count()\n",
        "print(f\"Number of rows before cleaning: {rows_before_cleaning}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvQRwDyXn3bR"
      },
      "outputs": [],
      "source": [
        "# Check the number of rows after cleaning\n",
        "rows_after_cleaning = df_cleaned.count()\n",
        "print(f\"Number of rows after cleaning: {rows_after_cleaning}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI7Vs-uXom12"
      },
      "source": [
        "##### V. Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_eoHyCgov3p"
      },
      "source": [
        "###### 5.1 Create Recency, Frequency, and Monetary (RFM) features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EPoTNb6pdiR"
      },
      "source": [
        "Convert InvoiceDate to DateType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOpzY_v2pK-X"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, to_date\n",
        "\n",
        "# Convert InvoiceDate from string to DateType\n",
        "df_fe = df_cleaned.withColumn(\"InvoiceDate\", to_date(col(\"InvoiceDate\"), \"M/d/yyyy H:mm\"))\n",
        "df_fe.printSchema()\n",
        "df_fe.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2233J-crb1o"
      },
      "outputs": [],
      "source": [
        "# Get the max date of the dataset\n",
        "max_date = df_fe.agg(max(\"InvoiceDate\")).collect()[0][0]\n",
        "max_date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyIf_DT-ud8q"
      },
      "outputs": [],
      "source": [
        "# Calculate recency\n",
        "recency_df = df_fe.groupBy(\"CustomerID\").agg(\n",
        "    datediff(lit(max_date), max(\"InvoiceDate\")).alias(\"Recency\")\n",
        ")\n",
        "recency_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoR6spMQvwa9"
      },
      "outputs": [],
      "source": [
        "# Calculate frequency (number of transactions per customer)\n",
        "frequency_df = df_fe.groupBy(\"CustomerID\").agg(\n",
        "    countDistinct(\"InvoiceNo\").alias(\"Frequency\")\n",
        ")\n",
        "frequency_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAMEye-cwVJq"
      },
      "outputs": [],
      "source": [
        "# Calculate Monetary (Total money spent by customer)\n",
        "monetary_df = df_fe.withColumn(\"TotalPrice\", col(\"Quantity\") * col(\"UnitPrice\")) \\\n",
        "    .groupBy(\"CustomerID\") \\\n",
        "    .agg(round(sum(\"TotalPrice\"), 3).alias(\"Monetary\"))\n",
        "monetary_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3hVol3xw0NN"
      },
      "outputs": [],
      "source": [
        "# Join all RFM features\n",
        "dfs = [recency_df, frequency_df, monetary_df]\n",
        "rfm_df = reduce(lambda df1, df2: df1.join(df2, \"CustomerID\"), dfs)\n",
        "rfm_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Y4O0Ngaxw5M"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import mean\n",
        "\n",
        "rfm_df.select(\n",
        "    mean(\"Recency\").alias(\"Mean_Recency\"),\n",
        "    mean(\"Frequency\").alias(\"Mean_Frequency\"),\n",
        "    mean(\"Monetary\").alias(\"Mean_Monetary\")\n",
        ").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiQvdwbQyWTy"
      },
      "outputs": [],
      "source": [
        "# Define thresholds based on the mean values\n",
        "recency_threshold = 95\n",
        "frequency_threshold = 7\n",
        "\n",
        "# Create a new column 'Churn' based on the thresholds\n",
        "rfm_df = rfm_df.withColumn(\n",
        "    \"Churn\",\n",
        "    when((col(\"Recency\") > recency_threshold) & (col(\"Frequency\") <= frequency_threshold), 1).otherwise(0)\n",
        ")\n",
        "\n",
        "# Show the resulting DataFrame with Churn column\n",
        "rfm_df.select(\"CustomerID\", \"Recency\", \"Frequency\", \"Monetary\", \"Churn\").show(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGPVXIv2S-J3"
      },
      "outputs": [],
      "source": [
        "# join data\n",
        "final_df= df_fe.join(rfm_df,on= 'CustomerID', how= 'left')\n",
        "final_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHst6VG2SU4O"
      },
      "source": [
        "### Customer Churn & Value Analysis: A 3Ps-G Framework Approach\n",
        "- This analysis aims to understand customer churn behavior and uncover opportunities to drive long-term growth by applying the 3Ps-G Framework — a strategic lens that evaluates customer dynamics through four key dimensions: Place, People, Product, and Growth.\n",
        "- The 3P-G framework is not academic but has emerged from practical growth operating systems used by growth-stage startups and tech companies (inspired by frameworks like AARRR or McKinsey’s 7S). It gained popularity among product managers, data analysts, and growth teams as a way to structure root cause analysis around user problems or retention issues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSQ_GRVnSY_z"
      },
      "source": [
        "### Overall RFM by churn\n",
        "- This is also conducted through the 3Ps-G frameworks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk9UUdO4ygwO"
      },
      "outputs": [],
      "source": [
        "final_df.groupBy(\"Churn\") \\\n",
        "    .agg(\n",
        "        avg(\"Recency\").alias(\"Avg_Recency\"),\n",
        "        avg(\"Frequency\").alias(\"Avg_Frequency\"),\n",
        "        avg(\"Monetary\").alias(\"Avg_Monetary\"),\n",
        "        countDistinct(\"CustomerID\").alias(\"Num_Customers\")\n",
        "    ).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNlGqzHKi3mD"
      },
      "source": [
        "### Customer Behavior Breakdown By RFM metrics\n",
        "- Based on the data, churned customers account for nearly one-third of the total customer base (1,376 out of 4,335). Their behavior suggests a pattern of one-time purchases, as indicated by significantly higher recency and very low frequency and monetary values. Active users, on the other hand, outperformed churn customers in engagement time, number of purchases, and the revenue they brought in.\n",
        "\n",
        "- Avg_Recency: ~16 days (vs. ~197 days for churned)\n",
        "\n",
        "- Avg_Frequency: ~390 purchases (vs. ~2.3)\n",
        "\n",
        "- Avg_Monetary: ~$422,701 (vs. ~$963)\n",
        "\n",
        "These metrics showed that active customers spend approximately 43,800% more on average than churned ones — highlighting the urgent need of rententention strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls_DGcYtbh4g"
      },
      "source": [
        "### Are there certain products/StockCodes more common in orders with high Recency (long since last purchase)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGXh48sUmO_i"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, percentile_approx, first\n",
        "recency_threshold = final_df.select(\n",
        "    percentile_approx(\"Recency\", 0.75).alias(\"recency_75th\")\n",
        ").collect()[0][\"recency_75th\"]\n",
        "high_recency_customers = final_df.filter(col(\"Recency\") >= recency_threshold) \\\n",
        "    .select(['CustomerID'])\n",
        "high_recency_txns = high_recency_customers.join(final_df, on=\"CustomerID\", how=\"inner\")\n",
        "high_recency_products = high_recency_txns.groupBy(\"StockCode\") \\\n",
        "    .count() \\\n",
        "    .orderBy(col(\"count\").desc())\n",
        "product_desc = final_df.select(\"StockCode\", \"Description\") \\\n",
        "    .dropna() \\\n",
        "    .dropDuplicates([\"StockCode\"])\n",
        "\n",
        "high_recency_products_with_desc = high_recency_products.join(product_desc, on=\"StockCode\", how=\"left\")\n",
        "high_recency_products_with_desc.select(\"StockCode\", \"Description\", \"count\") \\\n",
        "    .show(20, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjfwf9-PESMw"
      },
      "source": [
        "### Summary Of Insights Of Long-Time Customers:\n",
        "- Old-time buyers are showing strong interest in aesthetically appealing,  and seasonal home decor and gift items with a vintage vibe.\n",
        "- Products like ALARM CLOCK BAKELIKE PINK, RED RETROSPOT SMALL MILK JUG, and BOX OF 6 MINI VINTAGE CRACKERS suggest a consistent preference for retro-style or vintage-themed items.\n",
        "- WOODLAND HEIGHT CHART STICKERS, CHRISTMAS STAR WISH LIST CHALKBOARD, and FELT TOADSTOOL LARGE indicate that buyers are actively purchasing home decor, likely for seasonal holiday such as Christmas.\n",
        "- TOADSTOOL MONEY BOX, OFFICE MUG WARMER CHOC+BLUE, and DOOR HANGER MUM + DADS ROOM suggests a high interest from office workers for a personalized, me-style everyday items such as mug\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X24JEMIewHlh"
      },
      "source": [
        "### Are there any popular products in customers with low recency (new customers):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lln6zctuwXmB"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, percentile_approx, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "recency_threshold = final_df.select(\n",
        "    percentile_approx(\"Recency\", 0.25).alias(\"recency_25th\")\n",
        ").collect()[0][\"recency_25th\"]\n",
        "\n",
        "low_recency_customers = final_df.filter(col(\"Recency\") <= recency_threshold) \\\n",
        "    .select(['CustomerID'])\n",
        "\n",
        "low_recency_txns = low_recency_customers.join(final_df, on=\"CustomerID\", how=\"inner\")\n",
        "\n",
        "low_recency_products = low_recency_txns.groupBy(\"StockCode\") \\\n",
        "    .count() \\\n",
        "    .orderBy(col(\"count\").desc())\n",
        "\n",
        "windowSpec = Window.partitionBy(\"StockCode\").orderBy(\"Description\")\n",
        "\n",
        "product_desc = final_df.select(\"StockCode\", \"Description\") \\\n",
        "    .dropna() \\\n",
        "    .withColumn(\"row_num\", row_number().over(windowSpec)) \\\n",
        "    .filter(col(\"row_num\") == 1) \\\n",
        "    .drop(\"row_num\")\n",
        "\n",
        "low_recency_products_with_desc = low_recency_products.join(product_desc, on=\"StockCode\", how=\"left\")\n",
        "\n",
        "low_recency_products_with_desc.select(\"StockCode\", \"Description\", \"count\") \\\n",
        "    .show(20, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jolmRlrHbsqE"
      },
      "source": [
        "### Are churn rates higher in certain countries?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HigcPFMipRbG"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, countDistinct, when\n",
        "churn_by_country = final_df.groupBy(\"Country\").agg(\n",
        "    countDistinct(when(col(\"Churn\") == 1, col(\"CustomerID\"))).alias(\"Churned_Customers\"),\n",
        "    countDistinct(when(col(\"Churn\") == 0, col(\"CustomerID\"))).alias(\"Active_Customers\")\n",
        ")\n",
        "\n",
        "churn_by_country = churn_by_country.withColumn(\n",
        "    \"Churn_Rate\",\n",
        "    col(\"Churned_Customers\") / (col(\"Churned_Customers\") + col(\"Active_Customers\"))\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtIi6UhspX3A"
      },
      "outputs": [],
      "source": [
        "churn_pd = churn_by_country.orderBy(col(\"Churn_Rate\").desc()).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eE0EmCnurvSX"
      },
      "outputs": [],
      "source": [
        "print(churn_pd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hn9U6KxgqPsF"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(data=churn_pd, x=\"Churn_Rate\", y=\"Country\", palette=\"Reds_r\")\n",
        "plt.title(\"Churn Rate by Country\")\n",
        "plt.xlabel(\"Churn Rate\")\n",
        "plt.ylabel(\"Country\")\n",
        "plt.grid(True, axis='x', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrCDaaTMr_F5"
      },
      "source": [
        "### Churn Rate Breakdown By Countries\n",
        "- United Kingdom — Largest Market: Customers: ~3,900 total (2706 active + 1211 churned) -> Main revenue driver, but churn rate is nearly 1/3 of total customers\n",
        "- Countries like Singapore, Saudi Arabia, Lithuania have NaN churn rates due to missing active or churned customers\n",
        "- Greece, Canada, Bahrain: Very high churn, but small customer base (1–3 active customers) — is this a outlier or data entry problem?\n",
        "- Unspecified-> subscious entry-> maybe a data entry issue\n",
        "- Austria: mid-size market, but notable churn rate\n",
        "- Channel Islands: small size market, but notably high churn rate\n",
        "- Finland, Norway, Germany, Spain: low churn rate, and mid-size market"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adDL1mttcU4p"
      },
      "source": [
        "### 3. People"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qN4XfYYcWNJ"
      },
      "source": [
        "### Which churned customers had high past value? Can they be targeted for reactivation?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZnT2RervZb2"
      },
      "outputs": [],
      "source": [
        "quantiles = final_df.approxQuantile(\"Monetary\", [0.75], 0.01)\n",
        "high_value_threshold = quantiles[0]\n",
        "high_value_churned = final_df.filter(\n",
        "    (col(\"Churn\") == 1) & (col(\"Monetary\") >= high_value_threshold)\n",
        ")\n",
        "high_value_churned.select(\"CustomerID\", \"Recency\", \"Frequency\", \"Monetary\").show(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAinXmGIUi1Z"
      },
      "source": [
        " - No churned customers have past high values -> 1. they almost leave after 1 and second purchase maybe because of customer service or product-related problems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HLh7JkFXafJ"
      },
      "source": [
        "### Which countries contribute the most revenue?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_BADjiP2XIg"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, sum as _sum\n",
        "\n",
        "# Compute revenue per country\n",
        "revenue_by_country_df = final_df.withColumn(\"Revenue\", col(\"Quantity\") * col(\"UnitPrice\")) \\\n",
        "    .groupBy(\"Country\") \\\n",
        "    .agg(_sum(\"Revenue\").alias(\"Total_Revenue\")) \\\n",
        "    .orderBy(col(\"Total_Revenue\").desc())\n",
        "revenue_pd = revenue_by_country_df.toPandas()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFzHolEE9wLZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.bar(revenue_pd[\"Country\"], revenue_pd[\"Total_Revenue\"], color='skyblue')\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel(\"Country\")\n",
        "plt.ylabel(\"Total Revenue\")\n",
        "plt.title(\"Revenue per Country\")\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DizIV_eeZkuw"
      },
      "source": [
        "- The United Kingdom outperforming other countries in revenue-> suggesting a strong customer base in this country\n",
        "- The revenue from the Netherlands, EIRE (Ireland), Germany, and France—while next in line—is negligible in comparison to the UK.\n",
        "- Most top-performing countries are concentrated in the Europe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl1AIBD_Hcty"
      },
      "source": [
        "### 2. The YOY growth in revenue of each countries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zSp3BatTn9e"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import year, lag, round\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "df_with_year = final_df.withColumn(\"Year\", year(\"InvoiceDate\"))\n",
        "# revenue per year\n",
        "revenue_per_year = df_with_year.groupBy(\"Country\", \"Year\") \\\n",
        "    .agg(Fsum(\"Monetary\").alias(\"YearlyRevenue\"))\n",
        "\n",
        "# yoy growth\n",
        "window_spec = Window.partitionBy(\"Country\").orderBy(\"Year\")\n",
        "\n",
        "revenue_growth = revenue_per_year.withColumn(\n",
        "    \"PreviousRevenue\", lag(\"YearlyRevenue\").over(window_spec)\n",
        ").withColumn(\n",
        "    \"YoY_Growth\", round(((col(\"YearlyRevenue\") - col(\"PreviousRevenue\")) / col(\"PreviousRevenue\")) * 100, 2)\n",
        ").orderBy(\"Country\", \"Year\")\n",
        "\n",
        "revenue_growth.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUjp0sBqUwr1"
      },
      "source": [
        "### YoY (Year Over Year Growth In Revenue) Walk Through:\n",
        "- Channel Islands: +72,425% YoY (leading growing market)-> target them with new customer campaign to turn them into loyal customers\n",
        "- Australia: +60,724% YoY (148K -> 90M) -> massive growing market\n",
        "- Bahrain: -99.58% YoY -> almost frozen market\n",
        "- Countries like Brazil, Canada, Czech Republic, European Community only start buying from us in 2011\n",
        "- Austria: +2,403% YoY, Denmark: +1,406% YoY and Ireland (EIRE): +1,723% YoY. Grow slightly better than next year-> customer feedback analysis needed to discover how we can target these markets better"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX6Ph0L3ZmTj"
      },
      "source": [
        "### Sales Trend Of Top 3 Countries In Revenue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDuKtpZC-l4V"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, sum as _sum\n",
        "\n",
        "# Compute total revenue per country\n",
        "top_countries_df = final_df.withColumn(\"Revenue\", col(\"Quantity\") * col(\"UnitPrice\")) \\\n",
        "    .groupBy(\"Country\") \\\n",
        "    .agg(_sum(\"Revenue\").alias(\"Total_Revenue\")) \\\n",
        "    .orderBy(col(\"Total_Revenue\").desc()) \\\n",
        "    .limit(3)\n",
        "\n",
        "top_countries = [row[\"Country\"] for row in top_countries_df.collect()]\n",
        "print(\"Top 3 Countries by Revenue:\", top_countries)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfnDjRo7-mV9"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import month, year\n",
        "\n",
        "# Filter only top 3 countries\n",
        "filtered_df = final_df.filter(col(\"Country\").isin(top_countries))\n",
        "\n",
        "# Compute monthly revenue\n",
        "monthly_revenue_trend = filtered_df.withColumn(\"Revenue\", col(\"Quantity\") * col(\"UnitPrice\")) \\\n",
        "    .withColumn(\"Year\", year(\"InvoiceDate\")) \\\n",
        "    .withColumn(\"Month\", month(\"InvoiceDate\")) \\\n",
        "    .groupBy(\"Year\", \"Month\", \"Country\") \\\n",
        "    .agg(_sum(\"Revenue\").alias(\"Total_Revenue\")) \\\n",
        "    .orderBy(\"Year\", \"Month\", \"Country\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PICyCd7T-n85"
      },
      "outputs": [],
      "source": [
        "# Convert to Pandas\n",
        "monthly_pd = monthly_revenue_trend.toPandas()\n",
        "monthly_pd[\"YearMonth\"] = monthly_pd[\"Year\"].astype(str) + \"-\" + monthly_pd[\"Month\"].astype(str).str.zfill(2)\n",
        "\n",
        "# Sort by time\n",
        "monthly_pd = monthly_pd.sort_values([\"Year\", \"Month\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLIjv_DP-pd1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "for country in top_countries:\n",
        "    country_data = monthly_pd[monthly_pd[\"Country\"] == country]\n",
        "    plt.plot(country_data[\"YearMonth\"], country_data[\"Total_Revenue\"], label=country, marker='o')\n",
        "\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Monthly Revenue Trend of Top 3 Countries\")\n",
        "plt.xlabel(\"Year-Month\")\n",
        "plt.ylabel(\"Total Revenue\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS2q_yHkY1TX"
      },
      "source": [
        "- The UK is the top 1 performer in revenue, and revenue of this country suddenly spike in 11/2011 and significantly decrease in the next month-> a season purchasing behavior, strongly linked to\n",
        "- Other countries in top 3 revenue often have a plateu-like purchasing pattern, with a slightly increase in mid-year period (7-8/2011)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwHsbw34Xmo2"
      },
      "source": [
        "### What are the top 10 best-selling products by quantity?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNOZuvpKAAq6"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import sum as _sum\n",
        "top_products = final_df.groupBy(\"Description\") \\\n",
        "    .agg(_sum(\"Quantity\").alias(\"TotalQuantity\")) \\\n",
        "    .orderBy(\"TotalQuantity\", ascending=False)\n",
        "top_products.show(10, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wR6Qo4FYHw-"
      },
      "source": [
        "- The top-selling product is \"PAPER CRAFT , LITTLE BIRDIE\" with over 80,000 units sold, indicating a strong customer interest in affordable, decorative, and possibly DIY-themed items\n",
        "- Other best selling rpoducts such as ceramic jar, or asstd designs, or other decorations-> customers have preference for unique, niche decorative products."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhIjoNhPcF0k"
      },
      "source": [
        "### Which products churned customers mostly buy and when they buy them?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4hXGTA8bxPV"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, sum as _sum, desc, month, year, to_date\n",
        "churned_df = final_df.filter(col(\"Churn\") == 1)\n",
        "\n",
        "top_products = churned_df.groupBy(\"StockCode\", \"Description\") \\\n",
        "    .agg(_sum(\"Quantity\").alias(\"TotalQuantity\")) \\\n",
        "    .orderBy(desc(\"TotalQuantity\")) \\\n",
        "    .limit(3)\n",
        "top_products.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEZv3_l8eq7H"
      },
      "source": [
        "- Worth-noticing is that, Stock Code: 23166 appears both in best-selling lists, and top purchasing for churn customers\n",
        "- Other products are also fragile, and decorative items-> investigate the quality of products through customer feedbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-ozrc3Zho25"
      },
      "source": [
        "### 5. Growth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFe36MIDh6m1"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "product_df = final_df.withColumn(\"Year\", F.year(\"InvoiceDate\"))\n",
        "\n",
        "# Calculate total revenue per product per year\n",
        "product_year_revenue = product_df.groupBy(\"StockCode\", \"Description\", \"Year\") \\\n",
        "    .agg(F.sum(F.col(\"Quantity\") * F.col(\"UnitPrice\")).alias(\"YearlyRevenue\"))\n",
        "window_spec = Window.partitionBy(\"StockCode\").orderBy(\"Year\")\n",
        "\n",
        "# Get previous year's revenue to calculate YoY growth\n",
        "product_year_revenue = product_year_revenue.withColumn(\n",
        "    \"PreviousYearRevenue\", F.lag(\"YearlyRevenue\").over(window_spec))\n",
        "\n",
        "# Calculate YoY growth %\n",
        "product_year_revenue = product_year_revenue.withColumn(\n",
        "    \"YoY_Growth\",\n",
        "    (F.col(\"YearlyRevenue\") - F.col(\"PreviousYearRevenue\")) / F.col(\"PreviousYearRevenue\") * 100)\n",
        "product_year_revenue_filtered = product_year_revenue.filter(F.col(\"PreviousYearRevenue\").isNotNull())\n",
        "top_growth_products = product_year_revenue_filtered.orderBy(F.desc(\"YoY_Growth\"))\n",
        "top_growth_products.show(20, False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYN9rj62jFEY"
      },
      "source": [
        "- It is noticeble that specialize products for holidy events such as Christmas, are growing very fast. Our customers are also shifting their interest towards souvernirs, gifts, or special occassion items such as wedding day card."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}